{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCAAI Internship_Girija Soni_ problem statement I (1).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfFnSm9NGwE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "  # trainX=[]\n",
        "  # trainY=[]\n",
        "  # testX=[]\n",
        "  # testY=[]\n",
        "  (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "  # for item in range(5000):\n",
        "  #   trainX.append(train_X[item])\n",
        "  #   trainY.append(train_Y[item])\n",
        "  #   testX.append(test_X[item])\n",
        "  #   testY.append(test_Y[item])\n",
        "\t# one hot encode target values\n",
        "  trainY = to_categorical(trainY)\n",
        "  testY = to_categorical(testY)\n",
        "  return trainX, trainY, testX, testY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YZtznMAQ6Bj",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngWIGc8lJ-Nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a6670f99-288b-453e-c52d-f7ac42627630"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHEii4N7OiI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bd4c59-dfd4-40a8-c2cc-8a8e3cdf6726"
      },
      "source": [
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "  print(\"prepping pixels\")\n",
        "  train_norm = np.array(train, dtype=np.float32)\n",
        "  test_norm = np.array(test, dtype=np.float32)\n",
        "\t# normalize to range 0-1\n",
        "  train_norm = train_norm / 255.0\n",
        "  test_norm = test_norm / 255.0\n",
        "  # return normalized images\n",
        "  return train_norm, test_norm\n",
        " \n",
        "# define cnn model\n",
        "#The model consists of of 3 initial layers and uses a relu activation function\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "  # 20% of the nodes are removed using a dropout function to help regularization\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten()) #the layers a flattened out in this layer to pass into the next layer\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.001)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(10, activation='softmax')) #using a softmax activation in the final layer to \n",
        "\t# compile model\n",
        "\topt = SGD(lr=0.001, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test():\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\ttrainX, testX = prep_pixels(trainX, testX)\n",
        "\tmodel = define_model()\n",
        "\tmodel.fit(trainX, trainY, epochs=100, batch_size=64, verbose=0)#fit the made model into the data\n",
        "\tmodel.save('final_model.h5')\n",
        "\n",
        "run_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "prepping pixels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yqzmoT_UhbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "\n",
        "def load_image(filename):  #preparing the uploaded image \n",
        "\timg = load_img(filename, target_size=(32, 32))\n",
        "\timg = img_to_array(img)\n",
        "\tpyplot.imshow(img.reshape(32,32,3)/255,interpolation='nearest')\n",
        "\timg = img.reshape(1, 32, 32, 3)\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\treturn img\n",
        " \n",
        "def run_example(filename):\n",
        "\timg = load_image(filename)\n",
        "\tmodel = load_model('model.h5')\n",
        "\tresult = model.predict_classes(img)\n",
        "\treturn result[0]\n",
        "\n",
        "#Get all Labels\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "batches=unpickle(\"/content/batches.meta\")\n",
        "label_names=batches[b'label_names']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs0HbWz9cMw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image='/content/1.jfif'   #Change test image HERE to predict\n",
        "label_names[run_example(test_image)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}